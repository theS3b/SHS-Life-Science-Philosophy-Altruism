{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.wrappers import Wrapper\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=10,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            # Set this to True for maximum speed\n",
    "            see_through_walls=True,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"grand mission\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        # Create an empty grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        # Generate the surrounding walls\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Generate vertical separation wall\n",
    "        for i in range(0, height):\n",
    "            self.grid.set(5, i, Wall())\n",
    "        \n",
    "        # Place the door and key\n",
    "        self.grid.set(5, 6, Door(COLOR_NAMES[0], is_locked=True))\n",
    "        self.grid.set(3, 6, Key(COLOR_NAMES[0]))\n",
    "\n",
    "        # Place a goal square in the bottom-right corner\n",
    "        self.put_obj(Goal(), width - 2, height - 2)\n",
    "\n",
    "        # Place the agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"grand mission\"\n",
    "\n",
    "class CustomManualControl(ManualControl):\n",
    "    def key_handler(self, event):\n",
    "        # If the 't' key is pressed, execute the pickup action.\n",
    "        if event.key == 't':\n",
    "            print('here')\n",
    "            self.step(self.env.actions.pickup)\n",
    "        else:\n",
    "            # For other keys, fall back to the default behavior.\n",
    "            super().key_handler(event)\n",
    "\n",
    "\n",
    "\n",
    "class ActionBonus(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A wrapper that adds an exploration bonus to less visited (state,action) pairs.\n",
    "\n",
    "        Args:\n",
    "            env: The environment to apply the wrapper\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.counts = {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment with `action`.\"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        env = self.unwrapped\n",
    "        tup = (tuple(env.agent_pos), env.agent_dir, action)\n",
    "\n",
    "        # Get the count for this (s,a) pair\n",
    "        pre_count = 0\n",
    "        if tup in self.counts:\n",
    "            pre_count = self.counts[tup]\n",
    "\n",
    "        # Update the count for this (s,a) pair\n",
    "        new_count = pre_count + 1\n",
    "        self.counts[tup] = new_count\n",
    "\n",
    "        bonus = - np.sqrt(new_count)\n",
    "        reward += bonus\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 512, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None], dtype=torch.float32)).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/ppo/miniworld_gotoobj_tensorboard/20250226-220237_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 400      |\n",
      "|    ep_rew_mean     | 284      |\n",
      "| time/              |          |\n",
      "|    fps             | 274      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 400          |\n",
      "|    ep_rew_mean          | 238          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 228          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143561475 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.000235     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.55         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0072      |\n",
      "|    value_loss           | 24.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | 207         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010288627 |\n",
      "|    clip_fraction        | 0.0289      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 1.94e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.94        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00344    |\n",
      "|    value_loss           | 15.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | 188         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004940259 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 2.98e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 393         |\n",
      "|    ep_rew_mean          | 173         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012745479 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 2.15e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 394         |\n",
      "|    ep_rew_mean          | 162         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013453468 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 3.76e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    value_loss           | 4.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 395         |\n",
      "|    ep_rew_mean          | 152         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016688934 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 1.31e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.471       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    value_loss           | 0.935       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 396         |\n",
      "|    ep_rew_mean          | 145         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013763873 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 7.9e-05     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.533       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 396         |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014902051 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.000923    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.602       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    value_loss           | 0.95        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 397         |\n",
      "|    ep_rew_mean          | 132         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010986473 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.201       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0084     |\n",
      "|    value_loss           | 0.523       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 397         |\n",
      "|    ep_rew_mean          | 127         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008155969 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.581       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.193       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    value_loss           | 0.613       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 397          |\n",
      "|    ep_rew_mean          | 123          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013743344 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | 0.567        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.358        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 1.23         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 397         |\n",
      "|    ep_rew_mean          | 121         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004216995 |\n",
      "|    clip_fraction        | 0.00195     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.345       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 0.716       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 117         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011083055 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | -0.477      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0959      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00821    |\n",
      "|    value_loss           | 0.345       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 398          |\n",
      "|    ep_rew_mean          | 114          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075529916 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | 0.322        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0754       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00322     |\n",
      "|    value_loss           | 0.205        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 112         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008704808 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.162       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    value_loss           | 0.598       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 109         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006942734 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.091       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    value_loss           | 0.247       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 107         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011281557 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 106         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 183         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010473749 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0689      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 398          |\n",
      "|    ep_rew_mean          | 99.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 192          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068801483 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.85        |\n",
      "|    explained_variance   | -0.0234      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.97         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    value_loss           | 7.09         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 90.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011708975 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | -0.801      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00587    |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 83.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009568551 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.000846    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.195       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 79.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 220         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011967383 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00932    |\n",
      "|    value_loss           | 0.0858      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 398         |\n",
      "|    ep_rew_mean          | 76.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 230         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012049624 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000949    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | 73.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013045864 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1d961b0b910>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pdb import set_trace\n",
    "from time import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from gymnasium.spaces import Box, Dict\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "\n",
    "env = SimpleEnv(render_mode=\"rgb_array\") #gym.make(\"MiniGrid-Empty-16x16-v0\", render_mode=\"rgb_array\")\n",
    "env = ActionBonus(env)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "stamp = datetime.fromtimestamp(time()).strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1e4,\n",
    "    save_path=f\"./models/ppo/miniworld_gotoobj_{stamp}/\",\n",
    "    name_prefix=\"iter\",\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./logs/ppo/miniworld_gotoobj_tensorboard/\",\n",
    ")\n",
    "model.learn(\n",
    "    50000,\n",
    "    tb_log_name=f\"{stamp}\",\n",
    "    callback=checkpoint_callback,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'spaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     env,\n\u001b[0;32m     16\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,\n\u001b[0;32m     17\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# add the experiment time stamp\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/ppo/miniworld_gotoobj_20250226-220237/iter_500000_steps.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     24\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:740\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(data)\n\u001b[0;32m    739\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 740\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# put state_dicts back in place\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_parameters(params, exact_match\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:174\u001b[0m, in \u001b[0;36mPPO._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# Initialize schedules for policy/value clipping\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range \u001b[38;5;241m=\u001b[39m get_schedule_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range)\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:135\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_class \u001b[38;5;241m=\u001b[39m RolloutBuffer\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_class(\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps,\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_kwargs,\n\u001b[0;32m    134\u001b[0m )\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule, use_sde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs\n\u001b[0;32m    137\u001b[0m )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Warn when not using CPU with MlpPolicy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\common\\policies.py:891\u001b[0m, in \u001b[0;36mMultiInputActorCriticPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    873\u001b[0m     observation_space: spaces\u001b[38;5;241m.\u001b[39mDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    889\u001b[0m     optimizer_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m ):\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_arch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mortho_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_std_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_expln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquash_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshare_features_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\common\\policies.py:507\u001b[0m, in \u001b[0;36mActorCriticPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mortho_init \u001b[38;5;241m=\u001b[39m ortho_init\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor \u001b[38;5;241m=\u001b[39m share_features_extractor\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_features_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor\u001b[38;5;241m.\u001b[39mfeatures_dim\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\common\\policies.py:120\u001b[0m, in \u001b[0;36mBaseModel.make_features_extractor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_features_extractor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseFeaturesExtractor:\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to create a features extractor.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebge\\miniconda3\\envs\\shs-philo\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:294\u001b[0m, in \u001b[0;36mCombinedExtractor.__init__\u001b[1;34m(self, observation_space, cnn_output_dim, normalized_image)\u001b[0m\n\u001b[0;32m    291\u001b[0m extractors: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    293\u001b[0m total_concat_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, subspace \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_image_space(subspace, normalized_image\u001b[38;5;241m=\u001b[39mnormalized_image):\n\u001b[0;32m    296\u001b[0m         extractors[key] \u001b[38;5;241m=\u001b[39m NatureCNN(subspace, features_dim\u001b[38;5;241m=\u001b[39mcnn_output_dim, normalized_image\u001b[38;5;241m=\u001b[39mnormalized_image)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'spaces'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from minigrid.wrappers import ActionBonus\n",
    "from gymnasium.spaces import Box, Dict\n",
    "\n",
    "\n",
    "\n",
    "env = SimpleEnv(render_mode=\"human\")  #gym.make(\"MiniGrid-Empty-16x16-v0\", render_mode=\"human\")\n",
    "env = ActionBonus(env)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "loaded_model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# add the experiment time stamp\n",
    "loaded_model = loaded_model.load(f\"models/ppo/miniworld_gotoobj_20250226-220237/iter_500000_steps.zip\", env=env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "rewards = 0\n",
    "\n",
    "for i in range(2000):\n",
    "    action, _state = loaded_model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "    rewards += reward\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step: {i}\")\n",
    "        print(f\"Action: {action}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Test reward: {rewards}\")\n",
    "        obs, info = env.reset()\n",
    "        rewards = 0\n",
    "        continue\n",
    "\n",
    "print(f\"Test reward: {rewards}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shs-philo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
